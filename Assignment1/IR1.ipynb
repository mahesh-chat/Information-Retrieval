{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Imports"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from nltk.stem import PorterStemmer\r\n",
    "import os\r\n",
    "path=r'.\\document'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Reading Files"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "dataArr =[]\r\n",
    "noOfFiles = 0\r\n",
    "for fl in os.listdir(path):\r\n",
    "  flToRead = path + \"\\\\\" + fl\r\n",
    "  # print(flToRead)\r\n",
    "  text = open(flToRead,'r')\r\n",
    "  dataArr.append(text.read())\r\n",
    "  noOfFiles+=1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Case fold"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "nData = ''\r\n",
    "for i in range(len(dataArr)):\r\n",
    "  tm = dataArr[i].casefold()\r\n",
    "  nData+=tm"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tokenize"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "a = nData.split(' ')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def myCounter(list_of_strings):\r\n",
    "    d = {}\r\n",
    "    for string in list_of_strings:\r\n",
    "        if string not in d:\r\n",
    "            d.update({string: 1},)\r\n",
    "        else:\r\n",
    "            d[string] += 1\r\n",
    "\r\n",
    "    return d\r\n",
    "dWords = myCounter(a)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Sorting the tokens and deleting the top 3 stopwords"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "sortWords = sorted(dWords.items(), key=lambda x:x[1], reverse=True)\r\n",
    "del sortWords[0:3]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Porter Stemmer"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "#converting list of tuples to dictionary\r\n",
    "sortWords = dict(sortWords)\r\n",
    "#getting keys of dict to array\r\n",
    "l = []\r\n",
    "for i in sortWords.keys():\r\n",
    "  l.append(i)\r\n",
    "\r\n",
    "#stemming\r\n",
    "p = PorterStemmer()\r\n",
    "stems = []\r\n",
    "for it in l:\r\n",
    "  tmp = p.stem(it)\r\n",
    "  if tmp not in stems:\r\n",
    "    stems.append(tmp)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creating Inverted Index"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "fDict = {}\r\n",
    "for i in range(noOfFiles):\r\n",
    "  txthere = dataArr[i].lower()\r\n",
    "  for item in stems:\r\n",
    "    if item in txthere:\r\n",
    "      if item not in fDict:\r\n",
    "        fDict[item] = []\r\n",
    "      if item in fDict:\r\n",
    "        fDict[item].append(i+1)\r\n",
    "fDict"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'of': [1, 2, 3, 4],\n",
       " 'is': [1, 2, 3, 4],\n",
       " 'and': [1, 3, 4],\n",
       " 'that': [1, 2, 3, 4],\n",
       " 'are': [1, 2, 3, 4],\n",
       " 'an': [1, 2, 3, 4],\n",
       " 'to': [1, 2, 3, 4],\n",
       " 'document': [1, 2, 4],\n",
       " 'index': [1, 2, 3],\n",
       " 'term': [1, 2, 3, 4],\n",
       " 'as': [1, 2, 3, 4],\n",
       " 'boolean': [1],\n",
       " 'we': [1, 2, 3, 4],\n",
       " 'for': [1, 2, 3, 4],\n",
       " 'inform': [1, 2, 4],\n",
       " 'set': [1, 2],\n",
       " 'queri': [1, 3],\n",
       " 'or': [1, 2, 3, 4],\n",
       " 'retriev': [1, 4],\n",
       " 'model': [1],\n",
       " 'can': [1, 4],\n",
       " 'also': [1, 4],\n",
       " 'it': [1, 2, 3, 4],\n",
       " 'first': [1],\n",
       " 'adopt': [1],\n",
       " 'ir': [1, 2],\n",
       " 'model.': [1],\n",
       " 'thi': [1],\n",
       " 'method,': [1],\n",
       " 'make': [1],\n",
       " 'number': [1, 2],\n",
       " 'logic': [1],\n",
       " 'operators.': [1],\n",
       " 'retrieval,': [1],\n",
       " 'collect': [1],\n",
       " 'subset': [1],\n",
       " 'by': [1],\n",
       " 'itself.': [1],\n",
       " 'be': [1, 2, 3, 4],\n",
       " 'seen': [1],\n",
       " 'attribut': [1],\n",
       " 'say': [1],\n",
       " 'whether': [1],\n",
       " 'particular': [1],\n",
       " 'exist': [1],\n",
       " 'document.': [1],\n",
       " 'interpret': [1],\n",
       " 'set-theoretical.': [1],\n",
       " 'practice,': [1],\n",
       " 'express': [1],\n",
       " 'where': [1, 3],\n",
       " 'oper': [1],\n",
       " 'usual': [1],\n",
       " 'intersection,': [1],\n",
       " 'union,': [1],\n",
       " 'complement,': [1],\n",
       " 'operand': [1],\n",
       " 'terms.': [1],\n",
       " 'ha': [1, 2, 3, 4],\n",
       " 'method': [1, 3],\n",
       " 'hi': [1, 2, 4],\n",
       " 'on': [1, 2, 3, 4],\n",
       " 'invert': [2, 3],\n",
       " 'each': [2, 3],\n",
       " 'list': [2, 3],\n",
       " 'from': [2, 3],\n",
       " 'use': [2, 3, 4],\n",
       " 'data': [2],\n",
       " 'structur': [2],\n",
       " 'word': [2, 3],\n",
       " 'you': [2, 3],\n",
       " 'found': [2, 4],\n",
       " 'store': [2],\n",
       " 'map': [2],\n",
       " 'content,': [2],\n",
       " 'such': [2],\n",
       " 'numbers,': [2],\n",
       " 'locat': [2],\n",
       " 'documents.': [2],\n",
       " 'simpl': [2],\n",
       " 'words,': [2],\n",
       " 'hashmap': [2],\n",
       " 'like': [2],\n",
       " 'direct': [2],\n",
       " 'web': [2],\n",
       " 'page.': [2],\n",
       " 'allow': [2],\n",
       " 'fast,': [2],\n",
       " 'concurr': [2],\n",
       " 'processing.': [2],\n",
       " 'receiv': [2],\n",
       " 'independ': [2],\n",
       " 'list,': [2],\n",
       " 'which': [2],\n",
       " 'process': [2],\n",
       " 'when': [2],\n",
       " 'occur': [2],\n",
       " 'query.': [2],\n",
       " 'index,': [2],\n",
       " 'associ': [2],\n",
       " 'list.': [2],\n",
       " 'post': [3],\n",
       " 'phrase': [3],\n",
       " 'non-posit': [3],\n",
       " \"index'\": [3],\n",
       " 'posit': [3, 4],\n",
       " 'uniqu': [3],\n",
       " 'both': [3],\n",
       " 'answer': [3, 4],\n",
       " 'frequent': [3, 4],\n",
       " 'same': [3, 4],\n",
       " 'basic': [3, 4],\n",
       " 'differ': [3],\n",
       " 'former': [3],\n",
       " 'mere': [3],\n",
       " 'docid,': [3],\n",
       " 'wherea': [3],\n",
       " 'latter': [3],\n",
       " 'docid': [3],\n",
       " 'positions.': [3],\n",
       " 'may': [3],\n",
       " 'proxim': [3],\n",
       " 'questions.': [3],\n",
       " 'perform': [3],\n",
       " 'query,': [3],\n",
       " 'must': [3],\n",
       " 'still': [3],\n",
       " 'access': [3],\n",
       " 'entri': [3],\n",
       " 'individu': [3],\n",
       " 'term.': [3],\n",
       " 'before,': [3],\n",
       " 'begin': [3],\n",
       " 'with': [3],\n",
       " 'least': [3],\n",
       " 'work': [3],\n",
       " 'your': [3],\n",
       " 'way': [3],\n",
       " 'down': [3],\n",
       " 'possibl': [3],\n",
       " 'alternatives.': [3],\n",
       " 'instead': [3],\n",
       " 'just': [3],\n",
       " 'check': [3],\n",
       " 'dot,': [3],\n",
       " 'merg': [3],\n",
       " 'procedur': [3],\n",
       " 'recal': [4],\n",
       " 'relev': [4],\n",
       " 'item': [4],\n",
       " 'measur': [4],\n",
       " 'precis': [4],\n",
       " 'fraction': [4],\n",
       " 'have': [4],\n",
       " 'been': [4],\n",
       " 'most': [4],\n",
       " 'effectiveness.': [4],\n",
       " 'relevant.': [4],\n",
       " 'retrieved.': [4],\n",
       " 'those': [4],\n",
       " 'help': [4],\n",
       " 'user': [4],\n",
       " 'question.': [4],\n",
       " 'non-relev': [4],\n",
       " 'donâ€™t': [4],\n",
       " 'provid': [4],\n",
       " 'actual': [4],\n",
       " 'information.': [4],\n",
       " 'terminolog': [4],\n",
       " 'confus': [4],\n",
       " 'matrix': [4],\n",
       " 'defin': [4],\n",
       " 'these': [4],\n",
       " 'two': [4],\n",
       " 'measures.': [4],\n",
       " 'concentr': [4],\n",
       " 'evalu': [4],\n",
       " 'return': [4],\n",
       " 'true': [4],\n",
       " 'positives,': [4],\n",
       " 'ask': [4],\n",
       " 'what': [4],\n",
       " 'percentag': [4],\n",
       " 'how': [4],\n",
       " 'fals': [4],\n",
       " 'returned.': [4]}"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Result"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "searchWord = 'index'\r\n",
    "print(searchWord,' present in file no : ')\r\n",
    "if searchWord in fDict:\r\n",
    "  print(fDict[searchWord])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "index  present in file no : \n",
      "[1, 2, 3]\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit"
  },
  "interpreter": {
   "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}